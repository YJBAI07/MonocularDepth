{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12684738,"sourceType":"datasetVersion","datasetId":8016356},{"sourceId":12684962,"sourceType":"datasetVersion","datasetId":8016496}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, gc, cv2, torch, h5py, shutil\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom transformers import AutoImageProcessor, ZoeDepthForDepthEstimation\n\n# ===== 1) 设备与模型 =====\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_name = \"Intel/zoedepth-nyu-kitti\"\nprocessor = AutoImageProcessor.from_pretrained(model_name)\nmodel = ZoeDepthForDepthEstimation.from_pretrained(model_name).to(device).eval()\n\n# ===== 2) 数据路径与输出 =====\nmat_path = \"/kaggle/input/nyuv2-dataset/nyu_depth_v2_labeled.mat\"\nidx_list_path = \"/kaggle/input/nyu2-test/nyu2_test.txt\"\n\nOUT_ROOT = \"/kaggle/working/download/nyuv2_zoe\"\nout_pred_raw_npy   = os.path.join(OUT_ROOT, \"pred_raw_npy\")       # 原始米制预测\nout_pred_aligned   = os.path.join(OUT_ROOT, \"pred_aligned_npy\")   # （可选）scale-only 对齐\nout_gt_npy         = os.path.join(OUT_ROOT, \"gt_npy\")             # GT 米制\nout_gt_png16       = os.path.join(OUT_ROOT, \"gt_png16\")           # GT 毫米 16-bit PNG\nfor d in [out_pred_raw_npy, out_pred_aligned, out_gt_npy, out_gt_png16]:\n    os.makedirs(d, exist_ok=True)\n\n# ===== 3) 读 NYUv2 =====\nwith h5py.File(mat_path, 'r') as f:\n    images = f['images'][:]   # (1449, 3, 640, 480)\n    depths = f['depths'][:]   # (1449, 640, 480)\n# 转成 (480, 640, 3, N) 与 (480, 640, N)\nimages = np.transpose(images, (3, 2, 1, 0)).astype(np.uint8)\ndepths = np.transpose(depths, (2, 1, 0)).astype(np.float32)\nidx_list = np.loadtxt(idx_list_path).astype(int) - 1  # 1-based -> 0-based\n\n# ===== 4) 工具函数 =====\ndef npy_save(path, arr):\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    np.save(path, arr.astype(np.float32))\n\ndef mm_png_save(path, depth_m):\n    \"\"\"以毫米写 16-bit PNG（输入单位=米）\"\"\"\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    depth_mm = np.clip(depth_m * 1000.0, 0, 65535).astype(np.uint16)\n    cv2.imwrite(path, depth_mm)\n\ndef scale_match(pred, gt, mask):\n    \"\"\"每张图的纯缩放对齐（scale-only）\"\"\"\n    v = mask & np.isfinite(gt) & np.isfinite(pred) & (gt > 0)\n    if not np.any(v):\n        return 1.0\n    p, g = pred[v], gt[v]\n    den = float((p**2).sum())\n    return float((g * p).sum() / den) if den > 0 else 1.0\n\n# ===== 5) 主循环：只保存 预测 + GT =====\nsaved = 0\nfor idx in tqdm(idx_list, desc=\"Saving NYUv2 (ZoeDepth)\"):\n    try:\n        rgb = images[..., idx]         # (480,640,3) uint8\n        gt  = depths[..., idx]         # (480,640)   float32, 米\n        mask = (gt > 0) & np.isfinite(gt)\n\n        # 前向（Zoe 输出米制）\n        with torch.no_grad():\n            inputs = processor(images=Image.fromarray(rgb), return_tensors=\"pt\").to(device)\n            outputs = model(**inputs)\n            processed = processor.post_process_depth_estimation(\n                outputs, source_sizes=[(rgb.shape[0], rgb.shape[1])]\n            )\n            pred_metric = processed[0][\"predicted_depth\"].squeeze().detach().cpu().numpy().astype(np.float32)\n\n        # 尺寸对齐到 GT\n        if pred_metric.shape != gt.shape:\n            pred_metric = cv2.resize(pred_metric, (gt.shape[1], gt.shape[0]), interpolation=cv2.INTER_LINEAR)\n\n        # （可选）scale-only 对齐一份（用于快速对比；统一评测时用 raw + alignment=\"ls\"）\n        s = scale_match(pred_metric, gt, mask)\n        pred_aligned = (pred_metric * s).astype(np.float32)\n\n        stem = f\"{int(idx):04d}\"\n\n        # 保存预测（原始米制 + 对齐后米制）\n        npy_save(os.path.join(out_pred_raw_npy, f\"{stem}.npy\"), pred_metric)\n        npy_save(os.path.join(out_pred_aligned, f\"{stem}.npy\"), pred_aligned)  #可注释\n\n        # 保存 GT（米制 + 毫米 PNG）\n        npy_save(os.path.join(out_gt_npy,   f\"{stem}.npy\"), gt)\n        mm_png_save(os.path.join(out_gt_png16, f\"{stem}.png\"), gt)\n\n        saved += 1\n\n        # 清理\n        del rgb, gt, pred_metric, pred_aligned, inputs, outputs, processed, mask\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    except Exception:\n        # 静默跳过异常样本\n        continue\n\nprint(f\"[NYUv2 ZoeDepth SAVE] saved pairs: {saved} / {len(idx_list)}\")\n\n# ===== 6) 打包下载 =====\nzip_base = \"/kaggle/working/zoe_nyuv2_preds_and_gt\"\nshutil.make_archive(zip_base, \"zip\", OUT_ROOT)\nprint(f\"打包完成: {zip_base}.zip\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-14T21:53:26.034021Z","iopub.execute_input":"2025-08-14T21:53:26.034228Z","iopub.status.idle":"2025-08-14T22:04:00.380921Z","shell.execute_reply.started":"2025-08-14T21:53:26.034211Z","shell.execute_reply":"2025-08-14T22:04:00.379962Z"}},"outputs":[{"name":"stderr","text":"2025-08-14 21:53:39.513855: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755208419.748825      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755208419.815796      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4448c814df794227a0c7b5f86f09845d"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8a762d43e8048f7bd82a7663cd9383e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.38G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b025250533248898199412e4db0dec8"}},"metadata":{}},{"name":"stderr","text":"Saving NYUv2 (ZoeDepth): 100%|██████████| 666/666 [06:36<00:00,  1.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"[NYUv2 ZoeDepth SAVE] saved pairs: 666 / 666\n打包完成: /kaggle/working/zoe_nyuv2_preds_and_gt.zip\n","output_type":"stream"}],"execution_count":1}]}