{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12568056,"sourceType":"datasetVersion","datasetId":7936848}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U transformers==4.53.0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-04T19:47:22.519672Z","iopub.execute_input":"2025-08-04T19:47:22.519904Z","execution_failed":"2025-08-04T19:47:24.241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, gc, cv2, torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport torch\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom transformers import AutoImageProcessor, ZoeDepthForDepthEstimation\n\n#模型加载\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_name = \"Intel/zoedepth-nyu-kitti\"\nprocessor = AutoImageProcessor.from_pretrained(model_name)\nmodel = ZoeDepthForDepthEstimation.from_pretrained(model_name).to(device)\n\n# ========== 路径配置 ==========\nbase_dir = \"/kaggle/input/kitti-dataset/depth_selection/val_selection_cropped\"\nimg_dir = os.path.join(base_dir, \"image\")\ngt_dir = os.path.join(base_dir, \"groundtruth_depth\")\nK_dir = os.path.join(base_dir, \"intrinsics\")\n\nsave_dir = \"/kaggle/working/\"\nos.makedirs(save_dir, exist_ok=True)\nvis_dir = os.path.join(save_dir, \"visualizations\")\nos.makedirs(vis_dir, exist_ok=True)\npred_dir = os.path.join(save_dir, \"predicted_depth\")\nos.makedirs(pred_dir, exist_ok=True)\n\n# ========== 可视化 ==========\ndef save_vis(rgb, gt, pred, mask, save_path):\n    plt.figure(figsize=(15, 5))\n    valid = mask & (gt > 0) & np.isfinite(gt) & np.isfinite(pred)\n    all_depths = np.concatenate([gt[valid], pred[valid]])\n    vmin = np.percentile(all_depths, 2)\n    vmax = np.percentile(all_depths, 98)\n\n    plt.subplot(1, 3, 1)\n    plt.imshow(rgb)\n    plt.title(\"RGB\")\n    plt.axis(\"off\")\n\n    plt.subplot(1, 3, 2)\n    plt.imshow(gt, cmap='viridis', vmin=vmin, vmax=vmax)\n    plt.title(\"GT Depth\")\n    plt.colorbar(fraction=0.046, pad=0.04)\n    plt.axis(\"off\")\n\n    plt.subplot(1, 3, 3)\n    plt.imshow(pred, cmap='viridis', vmin=vmin, vmax=vmax)\n    plt.title(\"Predicted (Aligned)\")\n    plt.colorbar(fraction=0.046, pad=0.04)\n    plt.axis(\"off\")\n\n    plt.tight_layout()\n    plt.savefig(save_path)\n    plt.close()\n\n# ========== 评估函数 ==========\n\"\"\"\ndef compute_abs_rel(pred, gt, mask):\n    pred, gt = pred[mask], gt[mask]\n    return np.mean(np.abs(pred - gt) / gt)\n\"\"\"\n#去除杂质\ndef compute_abs_rel(pred, gt, mask):\n    valid = mask & (gt > 0)\n    pred, gt = pred[valid], gt[valid]\n    if len(gt) == 0:\n        return np.nan\n    return np.mean(np.abs(pred - gt) / gt)\n\ndef compute_delta1(pred, gt, mask):\n    valid = mask & (gt > 1e-6) & (pred > 1e-6)\n    pred, gt = pred[valid], gt[valid]\n    thresh = np.maximum(pred / gt, gt / pred)\n    return np.mean(thresh < 1.25) if len(thresh) > 0 else np.nan\n\ndef scale_match(pred, gt, mask):\n    pred, gt = pred[mask], gt[mask]\n    return np.sum(gt * pred) / np.sum(pred ** 2)\n\n# ========== 主处理流程 ==========\nresults = []\nimg_files = sorted([f for f in os.listdir(img_dir) if f.endswith(\".png\")])\n\nfor img_file in tqdm(img_files, desc=\"Evaluating\"):\n    try:\n        # 构建文件路径\n        rgb_path = os.path.join(img_dir, img_file)\n        gt_file = re.sub(\"image\", \"groundtruth_depth\", img_file, count=1)\n        gt_path = os.path.join(gt_dir, gt_file)\n        K_path = os.path.join(K_dir, img_file.replace(\".png\", \".txt\"))\n\n        # 加载 RGB\n        rgb = cv2.imread(rgb_path)\n        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n        input_image = Image.fromarray(rgb)\n\n        # 加载 GT（PNG 转 float）\n        gt_png = cv2.imread(gt_path, cv2.IMREAD_UNCHANGED).astype(np.float32)\n        gt_depth = gt_png / 256.0  # KITTI GT 深度单位为毫米，需转换为米\n        mask = gt_depth > 0\n\n        # 加载内参\n        with open(K_path, 'r') as f:\n            vals = list(map(float, f.read().split()))\n            fx, fy = vals[0], vals[4]\n            cx, cy = vals[2], vals[5]\n\n        K = torch.tensor([[fx, 0, cx],\n                          [0, fy, cy],\n                          [0, 0, 1]], dtype=torch.float32).unsqueeze(0).to(device)\n\n        # ZoeDepth 推理\n        inputs = processor(images=input_image, return_tensors=\"pt\").to(device)\n        #inputs[\"camera_intrinsics\"] = K\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        processed = processor.post_process_depth_estimation(outputs, source_sizes=[(rgb.shape[0], rgb.shape[1])])\n        pred_depth = processed[0][\"predicted_depth\"].squeeze().cpu().numpy()\n\n        # 对齐预测深度（scale-matching）\n        if pred_depth.shape != gt_depth.shape:\n            pred_depth = cv2.resize(pred_depth, (gt_depth.shape[1], gt_depth.shape[0]), interpolation=cv2.INTER_LINEAR)\n\n        scale = scale_match(pred_depth, gt_depth, mask)\n        pred_aligned = pred_depth * scale\n\n        # 计算评估指标\n        absrel = compute_abs_rel(pred_aligned, gt_depth, mask)\n        delta1 = compute_delta1(pred_aligned, gt_depth, mask)\n        results.append([img_file, absrel, delta1])\n        print(f\"[{img_file}] AbsRel: {absrel:.4f}, δ1: {delta1 * 100:.2f}%\")\n\n        # 保存可视化图\n        vis_path = os.path.join(vis_dir, img_file.replace(\".png\", \"_vis.png\"))\n        save_vis(rgb, gt_depth, pred_aligned, mask, vis_path)\n\n        # 保存预测深度图（可选）\n        np.save(os.path.join(pred_dir, img_file.replace(\".png\", \"_pred.npy\")), pred_aligned)\n\n        del rgb, gt_depth, pred_depth, pred_aligned, inputs, outputs\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    except Exception as e:\n        print(f\"跳过 {img_file}，错误：{e}\")\n        continue\n\n# ========== 保存评估结果 ==========\n\"\"\"\ndf = pd.DataFrame(results, columns=[\"image\", \"AbsRel\", \"Delta1\"])\ndf.to_csv(os.path.join(save_dir, \"eval_results.csv\"), index=False)\nprint(f\"\\n所有评估结果保存至：{os.path.join(save_dir, 'eval_results.csv')}\")\n\"\"\"\ndf = pd.DataFrame(results, columns=[\"image\", \"AbsRel\", \"Delta1\"])\nmean_absrel = df[\"AbsRel\"].mean()\nmean_delta1 = df[\"Delta1\"].mean()\nmean_row = pd.DataFrame([[\"mean\", mean_absrel, mean_delta1]], columns=[\"image\", \"AbsRel\", \"Delta1\"])\ndf = pd.concat([df, mean_row], ignore_index=True)\ndf.to_csv(os.path.join(save_dir, \"eval_results.csv\"), index=False)\nprint(f\"\\n所有评估结果保存至：{os.path.join(save_dir, 'eval_results.csv')}\")\n\n#打包\nimport shutil\nzip_pre_path = os.path.join(save_dir, \"predicted_depth.zip\")\nshutil.make_archive(base_name=zip_pre_path.replace('.zip', ''), format='zip', root_dir=pred_dir)\nprint(f\"\\n预测深度npy已打包至: {zip_pre_path}\")\nzip_vis_path = os.path.join(save_dir, \"visualizations.zip\")\nshutil.make_archive(base_name=zip_vis_path.replace('.zip', ''), format='zip', root_dir=vis_dir)\nprint(f\"打包完成: {zip_vis_path}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-04T19:47:24.242Z"}},"outputs":[],"execution_count":null}]}